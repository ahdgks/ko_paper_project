{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45mglCVB00kR"
      },
      "source": [
        "# 시나리오"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_s0pSGm01xf"
      },
      "source": [
        "### 1. DB에 논문 Full Text 데이터 저장\n",
        "\n",
        "1. **데이터 준비:**\n",
        "   - 논문의 전체 텍스트를 문장 단위로 분리합니다.(후에 문장 유사도 분석을 통해 앞뒤 문장을 포함할지 말지를 결정할 수 있음) 이를 위해 Python의 `nltk` 라이브러리 같은 자연어 처리 도구를 사용할 수 있습니다.\n",
        "\n",
        "2. **데이터베이스 설계:**\n",
        "   - MySQL 같은 관계형 데이터베이스를 사용하여 데이터를 저장합니다. 각 문장은 별도의 행으로 저장됩니다.\n",
        "   - 예를 들어, `papers` 테이블에 `id`, `paper_id`, `sentence` 컬럼을 만들 수 있습니다.\n",
        "\n",
        "3. **데이터 삽입:**\n",
        "   - Python을 사용하여 분리된 문장을 데이터베이스에 삽입합니다.\n",
        "\n",
        "### 2. BERT 모델을 이용한 유의어 추출\n",
        "\n",
        "1. **BERT 모델 Fine-Tuning:**\n",
        "   - 특정 도메인의 데이터(예: 논문 데이터)에 맞게 BERT 모델을 Fine-Tuning 합니다. 이를 위해 해당 도메인의 데이터셋이 필요합니다.\n",
        "   - Fine-Tuning은 일반적으로 대규모 컴퓨팅 자원을 필요로 합니다.\n",
        "\n",
        "2. **유의어 추출:**\n",
        "   - 사용자의 요청을 입력으로 받아, Fine-Tuned BERT 모델을 이용하여 유의어를 추출합니다.\n",
        "   - 추출된 유의어는 상위 5개 정도를 선택합니다.\n",
        "\n",
        "### 3. 유의어를 포함한 데이터 검색\n",
        "\n",
        "1. **검색 쿼리 구성:**\n",
        "   - BERT 모델로부터 얻은 유의어를 이용하여 검색 쿼리를 구성합니다.\n",
        "   - SQL의 `LIKE` 연산자나 정규 표현식을 사용하여 유의어를 포함하는 문장을 찾을 수 있습니다.\n",
        "\n",
        "2. **데이터 검색:**\n",
        "   - 구성된 쿼리를 실행하여 데이터베이스에서 해당 유의어를 포함하는 문장들을 검색합니다.\n",
        "\n",
        "### 4. 웹상에서 결과 출력\n",
        "\n",
        "1. **웹 인터페이스 개발:**\n",
        "   - Streamlit 등과 같은 웹 프레임워크를 사용하여 웹 인터페이스를 개발합니다.\n",
        "   - 사용자가 요청을 입력하고, 검색 결과를 보여주는 페이지를 구성합니다.\n",
        "\n",
        "2. **결과 표시:**\n",
        "   - 검색된 데이터를 사용자에게 표시합니다. 이때, 검색된 문장과 함께 해당 문장이 포함된 논문의 정보(예: 제목, 저자 등)를 함께 보여줄 수 있습니다.\n",
        "\n",
        "### 주의 사항\n",
        "\n",
        "- BERT 모델의 Fine-Tuning과 유의어 추출은 상당한 컴퓨팅 자원과 전문 지식을 필요로 합니다.\n",
        "- 데이터베이스 설계와 쿼리 최적화는 시스템의 성능에 큰 영향을 미칩니다.\n",
        "- 웹 인터페이스는 사용자 친화적이고 직관적이어야 합니다.\n",
        "\n",
        "이러한 시스템을 구축하기 위해서는 데이터베이스 관리, 자연어 처리, 머신 러닝, 웹 개발 등 다양한 기술이 필요합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9KUxrmD06aE"
      },
      "source": [
        "# 1. DB에 논문 Full Text 데이터 저장"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQybwa0g1Luu"
      },
      "source": [
        "## 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLYlMXsdaObC",
        "outputId": "0e2daedc-bb27-4566-cbe1-b615b5c4b378"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence_transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=b688892d67cb9abcaba170638115ba1ca446c82e83d4fad81652003e66e8f605\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: sentencepiece, sentence_transformers\n",
            "Successfully installed sentence_transformers-2.2.2 sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "# !pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQJID-yAqF2Y",
        "outputId": "d656281f-730f-42f3-b407-b531083965db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.10/dist-packages (2.0.23)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy) (3.0.1)\n"
          ]
        }
      ],
      "source": [
        "# pip install sqlalchemy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTddGaewXRjL"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers\n",
        "from transformers import AutoModel\n",
        "from numpy.linalg import norm\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "import sentence_transformers\n",
        "import sqlalchemy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4oQr2XfNxt4"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/koala_common/data3val.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "g8n1jmOnOiiR",
        "outputId": "862546d7-9a04-454f-8c53-31fc295c395f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1307.1855'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['article_id'][10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1L-CF7x_RmcE",
        "outputId": "a836a6d7-e9f5-4df0-9edf-6a048099ad6e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6436, 4)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-SYFWg6QCj-"
      },
      "source": [
        "## 데이터 가공"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fawEnw7wehrE"
      },
      "source": [
        "### df 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w8W2OD7ZWQo",
        "outputId": "4ae63af0-ab4b-4a7e-e1bb-fe9bfb0f75db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        Unnamed: 0  article_id  \\\n",
            "0                0  PMC3872579   \n",
            "1                1  PMC3770628   \n",
            "2                2  PMC5330001   \n",
            "3                3  PMC4386667   \n",
            "4                4  PMC4307954   \n",
            "...            ...         ...   \n",
            "120775      133209  PMC4410469   \n",
            "120776      133210  PMC3934811   \n",
            "120777      133212  PMC4754593   \n",
            "120778      133213  PMC5317293   \n",
            "120779      133214  PMC4672074   \n",
            "\n",
            "                                             article_text  \\\n",
            "0       a recent systematic analysis showed that in 20...   \n",
            "1       it occurs in more than 50 of patients and may ...   \n",
            "2       tardive dystonia td a rarer side effect after ...   \n",
            "3       lepidoptera include agricultural pests that th...   \n",
            "4       syncope is caused by transient diffuse cerebra...   \n",
            "...                                                   ...   \n",
            "120775  the anatomy of the scapular neck may have sign...   \n",
            "120776  a 48year old woman with a positive brca1 gene ...   \n",
            "120777  upton and mccomas first reported double crush ...   \n",
            "120778  acute respiratory distress syndrome ards is a ...   \n",
            "120779  cardiac resynchronization therapy crt is an im...   \n",
            "\n",
            "                                            abstract_text  length  \n",
            "0       [<S> background : the present study was carrie...   26834  \n",
            "1       [<S> backgroundanemia in patients with cancer ...   17137  \n",
            "2       [<S> tardive dystonia ( td ) is a serious side...    6983  \n",
            "3       [<S> many lepidopteran insects are agricultura...   34663  \n",
            "4       [<S> we present an unusual case of recurrent c...    6546  \n",
            "...                                                   ...     ...  \n",
            "120775  [<S> purpose : reverse total shoulder arthropl...   11753  \n",
            "120776  [<S> a 48-year - old woman with a positive brc...    6171  \n",
            "120777  [\"<S> double compression of the ulnar nerve , ...    7215  \n",
            "120778  [<S> backgroundno definitive conclusions have ...   25946  \n",
            "120779  [<S> backgroundvarious difficulties can occur ...   22732  \n",
            "\n",
            "[120780 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "# 작은 따옴표와 백슬래시를 제거하는 함수 정의\n",
        "def remove_quotes_and_backslash(text):\n",
        "    if isinstance(text, str):\n",
        "        return text.replace(\"'\", \"\").replace(\"\\\\\", \"\")\n",
        "    return text\n",
        "\n",
        "# 데이터프레임의 모든 요소에 함수 적용\n",
        "df_cleaned = df.applymap(remove_quotes_and_backslash)\n",
        "\n",
        "print(df_cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyLNb7fcbLTl"
      },
      "outputs": [],
      "source": [
        "# 'article_id' 열의 중복 제거\n",
        "df_unique = df_cleaned.drop_duplicates(subset='article_id')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asZEMYT0RIAX"
      },
      "outputs": [],
      "source": [
        "result_list = [(row['article_id'], row['abstract_text']) for index, row in df_unique.iterrows()] # paper.db의 paper테이블에 들어갈 데이터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jh5bjMaCRko8",
        "outputId": "82ad474b-0fbe-4f9e-af31-44f822a5a1ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "120780"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(result_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KP9A-48cekjf"
      },
      "outputs": [],
      "source": [
        "# key를 integer로 변경"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwyGBF12SEUP"
      },
      "source": [
        "### sentence 테이블에 들어갈 데이터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbtdQHV5cLDt",
        "outputId": "f5cfed7f-51ae-4613-9b0d-eaf32c449093"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "\n",
        "# nltk의 punkt tokenizer를 다운로드합니다. 이 코드는 한 번만 실행하면 됩니다.\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJEFLcZjcFf4"
      },
      "outputs": [],
      "source": [
        "def sentence_boundary_detection(text):\n",
        "    # 영어 문장 경계 인식기를 로드합니다\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "    # 텍스트를 문장 단위로 분할합니다\n",
        "    return tokenizer.tokenize(text)\n",
        "\n",
        "# 리스트 컴프리헨션을 사용하여 결과 생성\n",
        "sentence_result_list = [(article_id, sentence.strip())\n",
        "                        for article_id, text in zip(df_unique['article_id'], df_unique['article_text'])\n",
        "                        for sentence in sentence_boundary_detection(text)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA3V00xHcV7N",
        "outputId": "f62ca0b9-1721-4047-bea9-8f257ce75f9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('0708.1996',\n",
              " \"', 'the twisted nematic liquid crystal cell serves as an example .\")"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_result_list[1:3][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6SpaNdmqNrw"
      },
      "source": [
        "## (무시)임베딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 883
        },
        "id": "3lFTSw2EYXsV",
        "outputId": "cbff344a-16a7-4629-9507-0e5be8b1f63a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b37d48a40514fada2827907a8736f54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7399d81cfef4329b6de208af3451be3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "117e76116b3c431f948aba604a6f728c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/69.0k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ab2e2c15e814d19a3f5039accf9cae3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6425c49dfdb477ab3b990370decd080",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9431014d0884970b185f8d91ab44a60",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.mlmodel:   0%|          | 0.00/136k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5391ad1f83124faf879574aa07406057",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "weight.bin:   0%|          | 0.00/551M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c7eca46664884b208ee0c6e3a43baafc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)ml/float32_model.mlpackage/Manifest.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e70166bbfe246cf9308adea915d06a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4890954601b947e98c36e8bf6cb73164",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-w-mean-pooling.onnx:   0%|          | 0.00/547M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3abd32611fcf4ac6ae3a0e40040cd883",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.onnx:   0%|          | 0.00/547M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26612d3df8b541b6a2780bbdf7464f6b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/275M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62709c2997904d7490bb72eaf5290e34",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/275M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47871f55bd3e451e8e4f93b2cfb02d4d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96dd863a43fb4c89a943bc6c6a01f3a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fdf9e1f5a93347de85569e244958c622",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb6a754abe1f4678847d814b803df82a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/373 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f937fce429f4e7cb2fa046d6b2ece54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54b9cfa6fe164c36868bb204453dba97",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e53927420bf14f15a2ace4720f16c361",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "configuration_bert.py:   0%|          | 0.00/8.24k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/jinaai/jina-bert-implementation:\n",
            "- configuration_bert.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The repository for /root/.cache/torch/sentence_transformers/jinaai_jina-embeddings-v2-base-en/ contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co//root/.cache/torch/sentence_transformers/jinaai_jina-embeddings-v2-base-en/.\n",
            "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
            "\n",
            "Do you wish to run the custom code? [y/N] y\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c1d4111d3dd44db9f84b92e133a5f93",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modeling_bert.py:   0%|          | 0.00/97.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/jinaai/jina-bert-implementation:\n",
            "- modeling_bert.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        }
      ],
      "source": [
        "# from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# model = SentenceTransformer(\"jinaai/jina-embeddings-v2-base-en\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9UuajFeYkaR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "BMiRGanwOoNJ",
        "outputId": "2bee66a5-8dde-454d-dbe6-9e25d5905db2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ab2f11de0cd49458af8237e49e47a24",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Encoding:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# # model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
        "# input_data=df['article_text'][:10]\n",
        "\n",
        "# first_embeddings = []\n",
        "# for article in tqdm(input_data, desc=\"Encoding\"):\n",
        "#     first_embeddings.append(model.encode(article))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "54MBeXsAV-cO",
        "outputId": "d75ef034-ae65-4f1d-9f15-62d4fa8e9c65"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3553\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \u001b[1;32m\"<ipython-input-10-6ba43630021c>\"\u001b[0m, line \u001b[1;32m2\u001b[0m, in \u001b[1;35m<cell line: 2>\u001b[0m\n    text_list = ast.literal_eval(df['article_text'][0])\n",
            "  File \u001b[1;32m\"/usr/lib/python3.10/ast.py\"\u001b[0m, line \u001b[1;32m64\u001b[0m, in \u001b[1;35mliteral_eval\u001b[0m\n    node_or_string = parse(node_or_string.lstrip(\" \\t\"), mode='eval')\n",
            "\u001b[0;36m  File \u001b[0;32m\"/usr/lib/python3.10/ast.py\"\u001b[0;36m, line \u001b[0;32m50\u001b[0;36m, in \u001b[0;35mparse\u001b[0;36m\u001b[0m\n\u001b[0;31m    return compile(source, filename, mode, flags,\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m\"<unknown>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    a recent systematic analysis showed that in 2011 314 296 331 million children younger than 5 years were mildly moderately or severely stunted and 258 240 274 million were mildly moderately or severely underweight in the developing countries in iran a study among 752 high school girls in sistan and baluchestan showed prevalence of 162 86 and 15 for underweight overweight and obesity respectively the prevalence of malnutrition among elementary school aged children in tehran varied from 6 to 16 anthropometric study of elementary school students in shiraz revealed that 16 of them suffer from malnutrition and low body weight snack should have 300 400 kcal energy and could provide 5 10 g of protein day nowadays school nutrition programs are running as the national programs world wide national school lunch program in the united states there are also some reports regarding school feeding programs in developing countries in vietnam school base program showed an improvement in nutrient intakes in iran a national free food program nffp is implemented in elementary schools of deprived areas to cover all poor students however this program is not conducted in slums and poor areas of the big cities so many malnourished children with low socio economic situation are not covered by nffp although the rate of poverty in areas known as deprived is higher than other areas many students in deprived areas are not actually poor and can afford food hence nutritional value of the nffp is...\n\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ...\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers\n"
          ]
        }
      ],
      "source": [
        "# import ast\n",
        "# text_list = ast.literal_eval(df['article_text'][0])\n",
        "# text_list\n",
        "\n",
        "# split_lists = [item.split(\"', '\") for item in text_list]\n",
        "# sub_list=[]\n",
        "# # 결과 출력\n",
        "# for sublist in split_lists:\n",
        "#     sub_list.append(sublist)\n",
        "# sub_list[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R20Iht6HqQNx"
      },
      "source": [
        "## (무시) DB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE9k1TgGqhE5"
      },
      "source": [
        "### MySQL 데이터베이스 연결:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwBZVWGva-ID"
      },
      "outputs": [],
      "source": [
        "from sqlalchemy import create_engine\n",
        "\n",
        "# MySQL 연결을 위한 예시 연결 문자열\n",
        "# 이 정보는 실제 데이터베이스 설정에 맞게 수정해야 합니다.\n",
        "mysql_connection_string = 'mysql+pymysql://username:password@localhost/mydatabase'\n",
        "engine = create_engine(mysql_connection_string)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJfNAywXqiag"
      },
      "source": [
        "### 데이터프레임을 MySQL에 저장:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRmiQYQJqe_Q"
      },
      "outputs": [],
      "source": [
        "df.to_sql('my_table', con=engine, if_exists='replace', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKtvqytarDgz"
      },
      "source": [
        "### 데이터프레임을 MySQL에 저장:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoSNGZ3trJmw"
      },
      "source": [
        "`my_table`은 데이터를 저장할 테이블 이름입니다.  \n",
        "`if_exists` 옵션은 테이블이 이미 존재할 경우 어떻게 처리할지 결정합니다.\n",
        "'replace'는 기존 테이블을 대체하고, 'append'는 데이터를 추가합니다.  \n",
        "`index=False`는 데이터프레임의 인덱스를 테이블에 저장하지 않도록 합니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCYw7bySrEM5"
      },
      "outputs": [],
      "source": [
        "df.to_sql('my_table', con=engine, if_exists='replace', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLDd66dhyjQ2"
      },
      "source": [
        "## SQLite3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F3P6H7I2re1"
      },
      "source": [
        "### 데이터베이스 및 테이블 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEB1vdx22l6d"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "\n",
        "# 데이터베이스 연결 생성\n",
        "conn = sqlite3.connect('papers.db')\n",
        "\n",
        "# 커서 생성\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# papers 테이블 생성\n",
        "cursor.execute('''\n",
        "CREATE TABLE IF NOT EXISTS papers (\n",
        "    paper_id TEXT PRIMARY KEY,\n",
        "    abstract TEXT\n",
        ")\n",
        "''')\n",
        "\n",
        "# sentences 테이블 생성\n",
        "cursor.execute('''\n",
        "CREATE TABLE IF NOT EXISTS sentences (\n",
        "    sentence_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    paper_id TEXT,\n",
        "    sentence_text TEXT,\n",
        "    FOREIGN KEY (paper_id) REFERENCES papers (paper_id)\n",
        ")\n",
        "''')\n",
        "\n",
        "# 변경사항 커밋 및 연결 종료\n",
        "conn.commit()\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-HpVeCV2t-V"
      },
      "source": [
        "### 데이터 삽입"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NSGh0WW2mWn"
      },
      "outputs": [],
      "source": [
        "# 예시 데이터\n",
        "papers_data = [\n",
        "    (1, '논문 제목 1', '2023-01-01'),\n",
        "    (2, '논문 제목 2', '2023-01-02')\n",
        "]\n",
        "\n",
        "sentences_data = [\n",
        "    (1, '논문 1의 첫 번째 문장입니다.'),\n",
        "    (1, '논문 1의 두 번째 문장입니다.'),\n",
        "    (2, '논문 2의 첫 번째 문장입니다.'),\n",
        "    (2, 'This is second sentece of paper no. 2.'),\n",
        "    (2, 'This is third sentence of research paper no.2.')\n",
        "]\n",
        "\n",
        "# 데이터베이스에 데이터 삽입\n",
        "conn = sqlite3.connect('papers.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "cursor.executemany('INSERT INTO papers (paper_id, abstract) VALUES (?, ?)', result_list)\n",
        "cursor.executemany('INSERT INTO sentences (paper_id, sentence_text) VALUES (?, ?)', sentence_result_list)\n",
        "\n",
        "conn.commit()\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MNImPBJ2wpP"
      },
      "source": [
        "### 데이터 조회"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqONammBWOHn",
        "outputId": "a71d8120-d6dc-4a59-c6ee-10ac1ace8570"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/koala_common\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/koala_common"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEjLh1Dj4z9c"
      },
      "source": [
        "#### (무시) paper.db 전체 데이터 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPOYIsE42xH5"
      },
      "outputs": [],
      "source": [
        "conn = sqlite3.connect('papers.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# 모든 논문 조회\n",
        "cursor.execute('SELECT * FROM papers')\n",
        "papers = cursor.fetchall()\n",
        "for paper in papers:\n",
        "    print(paper)\n",
        "\n",
        "# 특정 논문의 문장 조회\n",
        "paper_id = 1\n",
        "cursor.execute('SELECT * FROM sentences WHERE paper_id = ?', (paper_id,))\n",
        "sentences = cursor.fetchall()\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n",
        "\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t581TIBv41m3"
      },
      "source": [
        "#### paper.db에서 원하는 단어 포함한 행 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "yRtbpLld3rXT",
        "outputId": "68ea272c-e560-4d0d-d219-a400cca85290"
      },
      "outputs": [
        {
          "ename": "OperationalError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-aa273b1050df>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# '논문' 키워드를 포함하는 문장 조회\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%cardi%'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SELECT * FROM sentences WHERE sentence_text LIKE ?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# 조회 결과 출력\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperationalError\u001b[0m: no such table: sentences"
          ]
        }
      ],
      "source": [
        "import sqlite3\n",
        "\n",
        "# 데이터베이스 연결\n",
        "conn = sqlite3.connect('papers.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# '논문' 키워드를 포함하는 문장 조회\n",
        "keyword = '%cardi%'\n",
        "cursor.execute('SELECT * FROM sentences WHERE sentence_text LIKE ?', (keyword,))\n",
        "\n",
        "# 조회 결과 출력\n",
        "sentences = cursor.fetchall()\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n",
        "\n",
        "# 연결 종료\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnX4Zw2q3wWN"
      },
      "source": [
        "# 2. BERT 모델을 이용한 유의어 추출"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO0H-s3n4KW2"
      },
      "source": [
        "* 이 코드는 toy 예제입니다. bert모델을 사용하지 않았으며 nltk 라이브러리를 활용해 간단하게 구현하였습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHeZKnAI3rrH",
        "outputId": "59d01dff-c424-47b4-c513-7125e5a35c8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMSOI4Xc4Pss",
        "outputId": "b1085cfc-bb16-49bd-e5ea-1398fa4b2191"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5cz9y4N4TYo",
        "outputId": "45392c97-98aa-4c42-e5fa-4f2cc57b9dcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['theme', 'report', 'newspaper', 'composition', 'paper']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "def find_synonyms(word, count=5):\n",
        "    synonyms = set()\n",
        "\n",
        "    for syn in wn.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name())\n",
        "            if len(synonyms) >= count:\n",
        "                return list(synonyms)[:count]\n",
        "\n",
        "    return list(synonyms)\n",
        "\n",
        "# 'paper'에 대한 유의어 찾기\n",
        "synonyms = find_synonyms('paper')\n",
        "print(synonyms)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Osh33IfE7O3o"
      },
      "source": [
        "# 3. 유의어를 포함한 데이터 검색"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gT9LP1hf4XpV",
        "outputId": "eaefa00f-ca3a-4062-cc0c-53f01538bd2f"
      },
      "outputs": [
        {
          "ename": "LookupError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-6beca52f5939>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# 'paper'에 대한 유의어 찾기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0msynonyms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_synonyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'disease'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# 유의어를 포함하는 문장 찾기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-6beca52f5939>\u001b[0m in \u001b[0;36mfind_synonyms\u001b[0;34m(word, count)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_synonyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msynonyms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msyn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msyn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0msynonyms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "import sqlite3\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "def find_synonyms(word, count=5):\n",
        "    synonyms = set()\n",
        "    for syn in wn.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name().replace('_', ' '))\n",
        "            if len(synonyms) >= count:\n",
        "                return list(synonyms)[:count]\n",
        "    return list(synonyms)\n",
        "\n",
        "def find_sentences_with_synonyms(synonyms):\n",
        "    conn = sqlite3.connect('papers.db')\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # 유의어를 포함하는 문장을 찾기 위한 쿼리 구성\n",
        "    query = \"SELECT * FROM sentences WHERE \" + \" OR \".join([\"sentence_text LIKE ?\" for _ in synonyms])\n",
        "    params = ['%' + synonym + '%' for synonym in synonyms]\n",
        "\n",
        "    cursor.execute(query, params)\n",
        "    results = cursor.fetchall()\n",
        "\n",
        "    conn.close()\n",
        "    return results\n",
        "\n",
        "# 'paper'에 대한 유의어 찾기\n",
        "synonyms = find_synonyms('disease')\n",
        "\n",
        "# 유의어를 포함하는 문장 찾기\n",
        "sentences = find_sentences_with_synonyms(synonyms)\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yDlMh6q7mGG"
      },
      "source": [
        "# 4. 웹상에서 결과 출력"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDIgdV_h7y7E"
      },
      "source": [
        "## 4.1. 필요한 라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "widJWibB40mg",
        "outputId": "95af7fe0-32dd-4b40-baa2-370e413283ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "pip install Flask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l45mFjkj7wcE"
      },
      "source": [
        "## 4.2. Flask 애플리케이션 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6ZB3Vgg9Urw"
      },
      "source": [
        "app.py라는 파일을 생성하고 다음 코드를 작성합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNNm_b-l7if7",
        "outputId": "9d568793-8d31-4afc-b1cc-1cfde12c10b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ],
      "source": [
        "# from flask import Flask, render_template, request\n",
        "# import sqlite3\n",
        "\n",
        "# app = Flask(__name__)\n",
        "\n",
        "# def get_search_results(keyword):\n",
        "#     conn = sqlite3.connect('papers.db')\n",
        "#     cursor = conn.cursor()\n",
        "\n",
        "#     # 검색 쿼리 실행\n",
        "#     query = '''\n",
        "#     SELECT sentences.sentence_text, papers.title\n",
        "#     FROM sentences\n",
        "#     JOIN papers ON sentences.paper_id = papers.paper_id\n",
        "#     WHERE sentences.sentence_text LIKE ?\n",
        "#     '''\n",
        "#     cursor.execute(query, ('%' + keyword + '%',))\n",
        "#     results = cursor.fetchall()\n",
        "\n",
        "#     conn.close()\n",
        "#     return results\n",
        "\n",
        "# @app.route('/', methods=['GET', 'POST'])\n",
        "# def index():\n",
        "#     search_results = []\n",
        "#     if request.method == 'POST':\n",
        "#         keyword = request.form['keyword']\n",
        "#         search_results = get_search_results(keyword)\n",
        "#     return render_template('index.html', search_results=search_results)\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     app.run(debug=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3vM5Ssk73UM"
      },
      "source": [
        "## 4.3. HTML 템플릿 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHyET96c9W58"
      },
      "source": [
        "Flask 애플리케이션과 같은 디렉토리에 templates라는 폴더를 만들고, 그 안에 index.html 파일을 생성합니다. 다음은 간단한 HTML 템플릿의 예시입니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOI1k6877s7T"
      },
      "outputs": [],
      "source": [
        "# <!DOCTYPE html>\n",
        "# <html>\n",
        "# <head>\n",
        "#     <title>논문 검색</title>\n",
        "# </head>\n",
        "# <body>\n",
        "#     <h1>논문 검색</h1>\n",
        "#     <form method=\"post\">\n",
        "#         <input type=\"text\" name=\"keyword\" placeholder=\"검색어 입력\">\n",
        "#         <button type=\"submit\">검색</button>\n",
        "#     </form>\n",
        "#     <hr>\n",
        "#     {% if search_results %}\n",
        "#         <h2>검색 결과</h2>\n",
        "#         <ul>\n",
        "#             {% for sentence, title in search_results %}\n",
        "#                 <li><strong>{{ title }}</strong>: {{ sentence }}</li>\n",
        "#             {% endfor %}\n",
        "#         </ul>\n",
        "#     {% else %}\n",
        "#         <p>검색 결과가 없습니다.</p>\n",
        "#     {% endif %}\n",
        "# </body>\n",
        "# </html>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFnbRNYq7-Ew"
      },
      "source": [
        "## 4.4. 애플리케이션 실행"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMPWnhVj9bgT"
      },
      "source": [
        "이제 애플리케이션을 실행할 수 있습니다. 터미널에서 다음 명령어를 실행하세요:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLcO1Ng79lCp",
        "outputId": "bf7a0e5d-d537-4bb9-d776-52e440e90489"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/koala_common\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/koala_common"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF3xxLi47-gn",
        "outputId": "68f39f17-2163-4ad4-d522-9012bbaf287a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app 'app'\n",
            " * Debug mode: on\n",
            "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            " * Restarting with stat\n",
            " * Debugger is active!\n",
            " * Debugger PIN: 995-851-124\n"
          ]
        }
      ],
      "source": [
        "!python app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD2LsqG3AWr4"
      },
      "source": [
        "# 남은 과제\n",
        "\n",
        "- 1.1 단계에서 문장 앞뒤로 연관된 내용인지 파악하여 추가하기.. ★★★(난이도) ♠(중요도)\n",
        "- 1.2 단계에서 DB 논리적 설계 다시하기(논문 정보) ★★ ♠♠\n",
        "- 1.3 단계에서 대용량 데이터 DB 삽입 자동화하기 ★ ♠♠♠\n",
        "- 2.1 단계에서 BERT모델 찾고, 파인튜닝(한국어 가능 모델) ★★★ ♠♠\n",
        "- 4단계에서 웹 연결 및 서비스 운영은 더 알아보기 ??? ♠♠♠\n",
        "\n",
        "# 우선순위\n",
        "<완료>\n",
        "- ~~ 1.2 단계에서 DB 논리적 설계 다시하기(논문 정보)~~  \n",
        " (가진 정보가 article_id, abstract, text뿐이어서 선택의 여지x)\n",
        "- ~~1.3 단계에서 대용량 데이터 DB 삽입 자동화하기~~\n",
        "---\n",
        "<2023-12-01일 전>\n",
        "\n",
        "- 2.1 단계에서 사용자의 입력에서 키워드를 추출해 유사어 5개를 찾아 줄 BERT모델 찾고, 우리 데이터에 맞게 파인튜닝(예: java-coffee/ java-python)\n",
        "- 문장 의미 단위 분할이 어려움..\n",
        "---\n",
        "<종강 이후>  \n",
        "- 4단계에서 웹 연결 및 서비스 운영은 더 알아보기\n",
        "- 1.1 단계에서 문장 앞뒤로 연관된 내용인지 파악하여 추가하기..\n",
        "\n",
        "# 일정\n",
        "- 마감: 12월 08일\n",
        "- 실질적 마감: 12월 01일\n",
        "-\n",
        "\n",
        "## 2023년 12월\n",
        "|     일     |     월     |     화     |     수     |     목     |     금     |     토     |\n",
        "|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|\n",
        "|            |            |            |            |            | 2023-12-01 | 2023-12-02 |\n",
        "|            |            |            |            |            |            |            |\n",
        "| 2023-12-03 | 2023-12-04 | 2023-12-05 | 2023-12-06 | 2023-12-07 | 2023-12-08 | 2023-12-09 |\n",
        "|<-|시|험|일|주|일|전|\n",
        "| 2023-12-10 | 2023-12-11 | 2023-12-12 | 2023-12-13 | **2023-12-14** | 2023-12-15 | 2023-12-16 |\n",
        "|->|데이터구조|            |딥러닝응용2|캡스톤마감 |            |            |\n",
        "| 2023-12-17 | 2023-12-18 | 2023-12-19 | 2023-12-20 | 2023-12-21 | 2023-12-22 | 2023-12-23 |\n",
        "|            |            |            |            |            |            |            |\n",
        "| 2023-12-24 | 2023-12-25 | 2023-12-26 | 2023-12-27 | 2023-12-28 | 2023-12-29 | 2023-12-30 |\n",
        "|            |            |            |            |            |            |            |\n",
        "| 2023-12-31 |            |            |            |            |            |\n",
        "           |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bhx523jRA7jR"
      },
      "source": [
        "# 시나리오 다시보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr6cLFpiA5cT"
      },
      "source": [
        "### 1. DB에 논문 Full Text 데이터 저장\n",
        "\n",
        "1. **데이터 준비:**\n",
        "   - 논문의 전체 텍스트를 문장 단위로 분리합니다.(후에 문장 유사도 분석을 통해 앞뒤 문장을 포함할지 말지를 결정할 수 있음) 이를 위해 Python의 `nltk` 라이브러리 같은 자연어 처리 도구를 사용할 수 있습니다.\n",
        "\n",
        "2. **데이터베이스 설계:**\n",
        "   - MySQL 같은 관계형 데이터베이스를 사용하여 데이터를 저장합니다. 각 문장은 별도의 행으로 저장됩니다.\n",
        "   - 예를 들어, `papers` 테이블에 `id`, `paper_id`, `sentence` 컬럼을 만들 수 있습니다.\n",
        "\n",
        "3. **데이터 삽입:**\n",
        "   - Python을 사용하여 분리된 문장을 데이터베이스에 삽입합니다.\n",
        "\n",
        "### 2. BERT 모델을 이용한 유의어 추출\n",
        "\n",
        "1. **BERT 모델 Fine-Tuning:**\n",
        "   - 특정 도메인의 데이터(예: 논문 데이터)에 맞게 BERT 모델을 Fine-Tuning 합니다. 이를 위해 해당 도메인의 데이터셋이 필요합니다.\n",
        "   - Fine-Tuning은 일반적으로 대규모 컴퓨팅 자원을 필요로 합니다.\n",
        "\n",
        "2. **유의어 추출:**\n",
        "   - 사용자의 요청을 입력으로 받아, Fine-Tuned BERT 모델을 이용하여 유의어를 추출합니다.\n",
        "   - 추출된 유의어는 상위 5개 정도를 선택합니다.\n",
        "\n",
        "### 3. 유의어를 포함한 데이터 검색\n",
        "\n",
        "1. **검색 쿼리 구성:**\n",
        "   - BERT 모델로부터 얻은 유의어를 이용하여 검색 쿼리를 구성합니다.\n",
        "   - SQL의 `LIKE` 연산자나 정규 표현식을 사용하여 유의어를 포함하는 문장을 찾을 수 있습니다.\n",
        "\n",
        "2. **데이터 검색:**\n",
        "   - 구성된 쿼리를 실행하여 데이터베이스에서 해당 유의어를 포함하는 문장들을 검색합니다.\n",
        "\n",
        "### 4. 웹상에서 결과 출력\n",
        "\n",
        "1. **웹 인터페이스 개발:**\n",
        "   - Flask나 Django 같은 Python 웹 프레임워크를 사용하여 웹 인터페이스를 개발합니다.\n",
        "   - 사용자가 요청을 입력하고, 검색 결과를 보여주는 페이지를 구성합니다.\n",
        "\n",
        "2. **결과 표시:**\n",
        "   - 검색된 데이터를 사용자에게 표시합니다. 이때, 검색된 문장과 함께 해당 문장이 포함된 논문의 정보(예: 제목, 저자 등)를 함께 보여줄 수 있습니다.\n",
        "\n",
        "### 주의 사항\n",
        "\n",
        "- BERT 모델의 Fine-Tuning과 유의어 추출은 상당한 컴퓨팅 자원과 전문 지식을 필요로 합니다.\n",
        "- 데이터베이스 설계와 쿼리 최적화는 시스템의 성능에 큰 영향을 미칩니다.\n",
        "- 웹 인터페이스는 사용자 친화적이고 직관적이어야 합니다.\n",
        "\n",
        "이러한 시스템을 구축하기 위해서는 데이터베이스 관리, 자연어 처리, 머신 러닝, 웹 개발 등 다양한 기술이 필요합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqvttbUSNEnw"
      },
      "outputs": [],
      "source": [
        "!pip install -q streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzFtxoAuNZqf",
        "outputId": "6bc3555c-67e2-462a-eff1-bc8909b23898"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "st.write('Hello, *World!* :sunglasses:')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55wp9FqRNcNF",
        "outputId": "6e0fceae-e1da-47d5-f765-af1952a462d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 4.374s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pv3HvT5RNdCj"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcf8PgTPNj2v",
        "outputId": "72d00734-2b3c-4b5a-b457-fbcb1d06e930"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 3.872s\n",
            "your url is: https://six-hotels-sneeze.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcMXRqMf9b_Y",
        "outputId": "827d0fa6-d41d-48bd-9a94-4ed92e52515f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-30 12:29:40.057 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n"
          ]
        }
      ],
      "source": [
        "import streamlit as st\n",
        "import sqlalchemy\n",
        "\n",
        "# 데이터베이스 연결 설정\n",
        "# 여기서는 예시로 SQLite를 사용합니다. 실제 사용 시에는 해당 데이터베이스에 맞게 조정해야 합니다.\n",
        "engine = sqlalchemy.create_engine('sqlite:///papers00.db')\n",
        "\n",
        "# Streamlit 인터페이스\n",
        "st.title(\"논문 레퍼런스 검색\")\n",
        "query = st.text_input(\"무엇을 도와드릴까요?\")\n",
        "\n",
        "if st.button(\"검색\"):\n",
        "    # 데이터베이스에서 쿼리 실행\n",
        "    results = engine.execute(f\"SELECT * FROM papers WHERE title LIKE '%{query}%'\")\n",
        "    # 결과 표시\n",
        "    for result in results:\n",
        "        st.write(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fxlHyHVM55Y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "MQybwa0g1Luu",
        "z6SpaNdmqNrw",
        "R20Iht6HqQNx",
        "hE9k1TgGqhE5"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}