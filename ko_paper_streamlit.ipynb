{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b493221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting streamlit_please.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ko_paper_streamlit.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from torch import torch\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    GenerationConfig,\n",
    "    pipeline,\n",
    ")\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import FAISS\n",
    "from torch import cuda, bfloat16\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.memory import StreamlitChatMessageHistory\n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "import torch\n",
    "from transformers import GenerationConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel,AutoPeftModelForCausalLM\n",
    "\n",
    "num=2\n",
    "adapter_dir = f'./results{num}/final_checkpoint'\n",
    "output_dir = f'./merged_peft{num}'\n",
    "output_merged_dir = os.path.join(output_dir, \"final_merged_checkpoint\")\n",
    "\n",
    "DEV = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model_name = output_merged_dir\n",
    "adapter_path = f'./results{num}/final_checkpoint'\n",
    "\n",
    "def main():\n",
    "    st.set_page_config(\n",
    "    page_title=\"DirChat\",\n",
    "    page_icon=\":books:\")\n",
    "    st.title(\"한국논문검색 챗봇 :red[한국어 전용] :books:\")\n",
    "    if \"conversation\" not in st.session_state:\n",
    "        st.session_state.conversation = None\n",
    "\n",
    "    if \"chat_history\" not in st.session_state:\n",
    "        st.session_state.chat_history = None\n",
    "\n",
    "    if \"processComplete\" not in st.session_state:\n",
    "        st.session_state.processComplete = None\n",
    "    \n",
    "    if \"model_loaded\" not in st.session_state:\n",
    "        model_name = 'intfloat/multilingual-e5-large'\n",
    "        model_kwargs = {\"device\": \"cuda\"}\n",
    "        encode_kwargs = {'normalize_embeddings': True}\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs=model_kwargs,\n",
    "            encode_kwargs=encode_kwargs\n",
    "        )\n",
    "        vector_db = Chroma(persist_directory=\"./version_10\", embedding_function=embeddings)\n",
    "\n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            adapter_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            quantization_config=bnb_config,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(output_merged_dir)\n",
    "        tokenizer.eos_token = '</s>'\n",
    "        pipe = pipeline(\"text-generation\", model=model, max_new_tokens=512, tokenizer=tokenizer, device_map=\"auto\")\n",
    "        llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "        st.session_state.model_loaded = True\n",
    "        st.session_state.vector_db = vector_db\n",
    "        st.session_state.llm = llm\n",
    "    else:\n",
    "        vector_db = st.session_state.vector_db\n",
    "        llm = st.session_state.llm\n",
    "\n",
    "    st.session_state.conversation = get_conversation_chain(vector_db, llm)\n",
    "    st.session_state.processComplete = True\n",
    "\n",
    "    if 'messages' not in st.session_state:\n",
    "        st.session_state['messages'] = [{\"role\": \"assistant\",\n",
    "                                         \"content\": \"안녕하세요! 찾고 싶은 논문이 있으면 ~~논문 찾아줘라고 요청해보세요!\"}]\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.markdown(message[\"content\"])\n",
    "    history = StreamlitChatMessageHistory(key=\"chat_messages\")\n",
    "    if query := st.chat_input(\"질문을 입력해주세요.\"):\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": query})\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.markdown(query)\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            chain = st.session_state.conversation\n",
    "            with st.spinner(\"요청한 논문을 찾는 중입니다...\"):\n",
    "                result = chain(query)\n",
    "                response = result['answer']\n",
    "                source_documents = result['source_documents']\n",
    "\n",
    "                st.markdown(response)\n",
    "\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "def get_conversation_chain(vector_db, llm):\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vector_db.as_retriever(search_type='mmr', search_kwargs={'k': 3, 'fetch_k': 3}, verbose=True),\n",
    "        memory=ConversationBufferMemory(memory_key='chat_history', return_messages=True, output_key='answer'),\n",
    "        get_chat_history=lambda h: h,\n",
    "        return_source_documents=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    conversation_chain.combine_docs_chain.llm_chain.prompt.template = \"\"\"너는 논문 검색 도우미야. 맥락을 참고해서 답해줘. 답변의 시작은 \"관련 논문은\"으로 시작해줘 .말투는 \"~~입니다.\"로 해.\n",
    "\n",
    "        {context}\n",
    "\n",
    "        질문: {question}\n",
    "\n",
    "        유용한 답변:\"\"\"\n",
    "\n",
    "    return conversation_chain\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e06e5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8dd6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run ko_paper_streamlit.py --server.port {'your_port'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahdgks",
   "language": "python",
   "name": "ahdgks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
