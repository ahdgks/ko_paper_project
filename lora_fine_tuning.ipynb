{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20d95b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-01 15:30:26.996365: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-01 15:30:27.013137: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-01 15:30:27.013155: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-01 15:30:27.013166: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-01 15:30:27.016438: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-01 15:30:27.327022: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import bitsandbytes as bnb\n",
    "import os\n",
    "import wandb\n",
    "import json\n",
    "\n",
    "from transformers import AdamW, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from tqdm import tqdm\n",
    "from peft import get_peft_model, PromptTuningConfig, TaskType, PromptTuningInit, PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments,BitsAndBytesConfig\n",
    "num=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d39202",
   "metadata": {},
   "source": [
    "# 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bfdc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('fine-tuning_dataset.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc462289",
   "metadata": {},
   "outputs": [],
   "source": [
    "#토큰화를 위해 토크나이저 선언하기\n",
    "tokenizer = AutoTokenizer.from_pretrained('LDCC/LDCC-SOLAR-10.7B',  eos_token='</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7cab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이 설정\n",
    "max_length = 660\n",
    "formatted_data = []\n",
    "input_texts = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    page_text = f\"### Question: {data['질문'][i]} {tokenizer.eos_token} \\n### Answer:{data['답변'][i]}\"\n",
    "    input_texts.append(page_text)\n",
    "\n",
    "for text in input_texts:\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt', padding='max_length', truncation=True, max_length=max_length)\n",
    "    formatted_data.append(input_ids)\n",
    "    \n",
    "\n",
    "print('Done.')\n",
    "print(len(formatted_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2e10d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_data = torch.cat(formatted_data, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881763e4",
   "metadata": {},
   "source": [
    "# 모델로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9649d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"LDCC/LDCC-SOLAR-10.7B\"\n",
    "output_dir=f\"results{num}\"\n",
    "\n",
    "#양자화 하는 코드\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                  torch_dtype=torch.float16, \n",
    "                                                  quantization_config=bnb_config, \n",
    "                                                  device_map={\"\":0})\n",
    "base_model.config.use_cache = False\n",
    "base_model = prepare_model_for_kbit_training(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14fbdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#토크나이저 이슈로 다시 선언해줘야함\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704016cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments,BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "#파라미터 갯수 세는 코드\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "# 로라 선언하는 코드\n",
    "config = LoraConfig(\n",
    "    r=16, \n",
    "    lora_alpha=8, \n",
    "    target_modules=[\n",
    "    \"q_proj\",\n",
    "    \"up_proj\",\n",
    "    \"o_proj\",\n",
    "    \"k_proj\",\n",
    "    \"down_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"v_proj\"],\n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, config)\n",
    "print_trainable_parameters(base_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db0923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습시키는 코드\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=formatted_data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size= 4, \n",
    "        gradient_accumulation_steps=1, \n",
    "        num_train_epochs=1,\n",
    "      #  max_steps=50,\n",
    "        learning_rate=3.5e-5,\n",
    "        max_grad_norm= 0.3,\n",
    "        fp16=True, # Use mixed precision(훈련 중에 모델에서 16-bit 및 32-bit 부동 소수점 유형을 모두 사용하여 더 빠르게 실행하고 메모리를 적게 사용하는 것입니다)\n",
    "        logging_steps=10, \n",
    "        output_dir=output_dir, \n",
    "        optim=\"paged_adamw_32bit\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(output_dir)\n",
    "\n",
    "output_dir = os.path.join(output_dir, \"final_checkpoint\")\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b540851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapter병합하는 코드 gpu메모리 문제로 커널 재시작 후 하는게 좋음, 아니면 gpu메모리 초기화하면됨\n",
    "import os\n",
    "import torch\n",
    "from transformers import GenerationConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel,AutoPeftModelForCausalLM\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# Update the path accordingly\n",
    "adapter_dir = f'./results{num}/final_checkpoint'\n",
    "output_dir = f'./merged_peft{num}'\n",
    "\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(adapter_dir, device_map=\"cuda\", torch_dtype=torch.float16)\n",
    "# generation_config.do_sample = True\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_dir)\n",
    "\n",
    "\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "\n",
    "\n",
    "output_merged_dir = os.path.join(output_dir, \"final_merged_checkpoint\")\n",
    "model.save_pretrained(output_merged_dir, safe_serialization=True,)\n",
    "tokenizer.save_pretrained(output_merged_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahdgks",
   "language": "python",
   "name": "ahdgks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
